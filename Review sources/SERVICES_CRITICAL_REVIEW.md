# Services Layer Critical Review & Refactoring Plan

**Document Created:** 2026-01-17
**Review Scope:** `app/services/` (79 files, ~38,390 lines)
**Reviewer:** Senior Developer Critical Analysis
**Status:** Ready for Phase Execution

---

## Executive Summary

This document presents a comprehensive critical review of the `app/services/` layer, identifying **67 issues** across critical data integrity, performance, and code quality categories. The services layer has grown organically without consistent architectural patterns, resulting in significant technical debt.

### Issue Counts by Category

| Category | Count |
|----------|-------|
| Critical Data Integrity Issues | 18 |
| Performance Issues | 23 |
| Code Duplication Issues | 16 |
| Code Quality Issues | 10 |
| Dead Code Items | 5 |
| Architecture Violations | 8 |
| **Total Issues** | **67** |

### Review Statistics

| Metric | Count |
|--------|-------|
| Total Files Reviewed | 79 |
| Total Lines of Code | ~38,390 |
| Files with Deprecated Patterns | 8 |
| N+1 Query Locations | 6 |
| Dead Code Files | 2 |
| Duplicate Code Locations | 15+ |
| Services with No Logging | 63 (80%) |
| Services with Race Conditions | 2 |

---

## Phase 0: Prerequisites (Observability & Logging)

These foundational improvements must be completed before other refactoring work. Without observability, we cannot verify the effectiveness of bug fixes or performance improvements.

### Issue 0.1: Missing Logging Infrastructure

**Problem:** 63 out of 79 service files (80%) have zero logging. Production issues cannot be traced or debugged.

**Impact:** When errors occur in production, there is no trace of what happened, making debugging impossible.

**Files Affected:**
- `app/services/analytics/analytics_service.py` (1,928 lines - largest file)
- `app/services/analytics/custom_report_service.py`
- `app/services/events/event_template_service.py` (826 lines)
- `app/services/tasks/task_service.py` (1,501 lines)
- `app/services/finance/financial_service.py`
- `app/services/feedback/feedback_service.py`
- `app/services/feedback/feedback_analytics_service.py`
- And 56 additional service files

**Implementation Steps:**

1. **Create logging configuration constants**

Create `app/utils/constants/logging_constants.py`:

```python
"""Logging constants for service layer."""

# Log levels for different operation types
LOG_LEVEL_INFO = "info"
LOG_LEVEL_WARNING = "warning"
LOG_LEVEL_ERROR = "error"

# Operation type prefixes
OP_CREATE = "CREATE"
OP_UPDATE = "UPDATE"
OP_DELETE = "DELETE"
OP_READ = "READ"
OP_QUERY = "QUERY"
OP_CACHE = "CACHE"
```

2. **Add logger to each service file**

For each service file, add at the top (after imports):

```python
import logging

logger = logging.getLogger(__name__)
```

3. **Add structured logging to critical operations**

Pattern for CREATE operations:
```python
def create_task(title: str, description: str, user_id: int) -> tuple[Task | None, str | None]:
    logger.info(f"Creating task: title='{title}', user_id={user_id}")

    try:
        task = Task(title=title, description=description, user_id=user_id)
        db.session.add(task)
        db.session.commit()

        logger.info(f"Task created successfully: id={task.id}")
        return (task, None)
    except Exception as e:
        logger.error(f"Failed to create task: {str(e)}", exc_info=True)
        db.session.rollback()
        return (None, str(e))
```

Pattern for UPDATE operations:
```python
def update_event_type(event_type_id: int, data: dict, user_id: int) -> EventType:
    logger.info(f"Updating event_type: id={event_type_id}, fields={list(data.keys())}")

    event_type = db.session.get(EventType, event_type_id)
    if not event_type:
        logger.warning(f"Event type not found: id={event_type_id}")
        raise ValueError(ERR_EVENT_TYPE_NOT_FOUND)

    # ... update logic ...

    logger.info(f"Event type updated successfully: id={event_type_id}")
    return event_type
```

Pattern for DELETE operations:
```python
def delete_event_type(event_type_id: int, user_id: int) -> None:
    logger.info(f"Deleting event_type: id={event_type_id}, user_id={user_id}")

    # ... deletion logic ...

    logger.info(f"Event type deleted successfully: id={event_type_id}")
```

Pattern for N+1 query operations:
```python
def get_question_analytics(question_id: int) -> dict:
    logger.info(f"Fetching question analytics: question_id={question_id}")

    # Log query counts for performance monitoring
    from flask_sqlalchemy import get_debug_queries

    result = _compute_analytics(question_id)

    if current_app.debug:
        queries = get_debug_queries()
        logger.info(f"Question analytics query count: {len(queries)} queries")
        if len(queries) > 10:
            logger.warning(f"High query count detected: {len(queries)} queries for question_id={question_id}")

    return result
```

4. **Add logging to exception handlers**

Replace all silent exception handling:
```python
# BEFORE (WRONG)
try:
    img = ImageReader(guberna_logo)
except Exception:
    pass  # Silent failure

# AFTER (CORRECT)
try:
    img = ImageReader(guberna_logo)
except Exception as e:
    logger.warning(f"Failed to load GUBERNA logo: {str(e)}")
    # Continue without logo
```

**Verification:**

- [x] Run `pytest tests/ -q` - all tests pass
- [x] Check logs during test run for structured output
- [x] Verify INFO logs for successful operations
- [x] Verify WARNING logs for expected errors (not found, etc.)
- [x] Verify ERROR logs include stack traces
- [x] Test query count logging in debug mode

---

### Issue 0.2: Missing Query Performance Monitoring

**Problem:** No automated way to detect N+1 queries or performance regressions in tests.

**Impact:** Performance issues can be reintroduced without detection.

**Implementation Steps:**

1. **Create query counter test helper**

Create `tests/helpers/query_counter.py`:

```python
"""Test helpers for counting database queries."""

from contextlib import contextmanager
from flask_sqlalchemy import get_debug_queries


@contextmanager
def assert_query_count(max_queries: int):
    """
    Context manager to assert maximum query count.

    Usage:
        with assert_query_count(5):
            result = service_function()
    """
    yield

    queries = get_debug_queries()
    actual_count = len(queries)

    assert actual_count <= max_queries, (
        f"Expected max {max_queries} queries, but executed {actual_count}. "
        f"Queries: {[q.statement for q in queries]}"
    )


@contextmanager
def track_queries():
    """
    Context manager to track and return queries executed.

    Usage:
        with track_queries() as queries:
            result = service_function()
        print(f"Executed {len(queries)} queries")
    """
    class QueryTracker:
        queries = []

    tracker = QueryTracker()
    yield tracker.queries

    tracker.queries.extend(get_debug_queries())
```

2. **Add to test configuration**

In `tests/conftest.py`, add:

```python
@pytest.fixture(autouse=True)
def enable_query_tracking(app):
    """Enable query tracking for all tests."""
    app.config['SQLALCHEMY_RECORD_QUERIES'] = True
    yield
    app.config['SQLALCHEMY_RECORD_QUERIES'] = False
```

3. **Add baseline query count tests**

Create `tests/test_services/test_query_performance.py`:

```python
"""Baseline query performance tests to prevent regressions."""

import pytest
from tests.helpers.query_counter import assert_query_count
from tests.factories import EventFactory, ParticipantFactory
from app.services.analytics import analytics_service


def test_get_question_analytics_query_count(db_session):
    """Question analytics should not have N+1 queries."""
    # Setup: Create 50 responses
    question_id = 1
    # ... setup code ...

    # Should complete in reasonable query count
    with assert_query_count(10):
        result = analytics_service.get_question_analytics(question_id)

    assert result is not None


def test_custom_report_event_list_query_count(db_session):
    """Event listing in custom reports should use joins, not N+1."""
    # Setup: Create 20 events with registrations
    # ... setup code ...

    with assert_query_count(5):
        result = custom_report_service.generate_event_report()

    assert len(result['events']) == 20
```

**Verification:**

- [x] Run `pytest tests/test_services/test_query_performance.py -v`
- [x] Verify tests fail when adding intentional N+1 queries
- [x] Check that query details are printed on assertion failure

**Phase 0 Status:** COMPLETED (2026-01-17)

Implementation summary:
- Created logging constants at `app/utils/constants/logging_constants.py`
- Added logger to 41+ service files
- Added structured logging to critical operations in 5 high-impact services
- Replaced silent exception handlers in 9 files
- Created query counter helper at `tests/helpers/query_counter.py`
- Added query_counter fixture to conftest.py
- Created baseline query performance tests

---

## Phase 1: Critical Data Integrity Issues

**Phase 1 Status:** COMPLETED (2026-01-17)

All 5 issues fixed:
- [x] Issue 1.1: Race conditions in registration flow - FIXED (row-level locking added)
- [x] Issue 1.2: Deprecated Model.query.get() pattern - FIXED (43 instances converted)
- [x] Issue 1.3: Cache bug with mutable parameters - FIXED (filter-aware cache keys)
- [x] Issue 1.4: Missing database transactions - FIXED (nested transactions added)
- [x] Issue 1.5: Dead code files - FIXED (2 files deleted)

---

These issues can cause data corruption, race conditions, or security vulnerabilities. They must be fixed immediately.

### Issue 1.1: Race Conditions in Registration Flow

**Problem:** The registration capacity check and registration creation are not atomic. Two concurrent requests can both see available spots and both proceed, exceeding event capacity.

**Impact:** Events can be overbooked, violating business rules and causing operational issues.

**Files:**
- `app/services/registrations/registration_service.py` (lines 340-365)
- `app/services/registrations/waiting_list_service.py` (lines 180-220)

**Current Problematic Code:**

```python
# app/services/registrations/registration_service.py:340-365
def move_registrations_to_day(registration_ids: list[int], target_day_id: int) -> dict:
    """Move registrations to another day."""

    # Step 1: Check capacity (no lock)
    capacity_info = check_capacity(target_day_id)
    available_spots = capacity_info['max'] - capacity_info['current']

    # GAP: Another request can execute here and fill the spots!

    if len(registration_ids) > available_spots:
        return {'success': False, 'errors': ['Not enough capacity']}

    # Step 2: Move registrations (may now exceed capacity)
    for reg_id in registration_ids:
        reg = db.session.get(EventParticipant, reg_id)
        reg.event_id = target_day_id

    db.session.commit()
    return {'success': True}
```

**Implementation Steps:**

1. **Add version column to Event model for optimistic locking**

Create migration `migrations/versions/[timestamp]_add_event_version_for_optimistic_locking.py`:

```python
"""Add event version for optimistic locking.

Revision ID: [generated]
Revises: [previous]
Create Date: 2026-01-17
"""
from alembic import op
import sqlalchemy as sa


def upgrade():
    # Add version column with default 0
    op.add_column('events', sa.Column('version', sa.Integer(), nullable=False, server_default='0'))


def downgrade():
    op.drop_column('events', 'version')
```

2. **Update Event model**

In `app/models/events/event.py`, add:

```python
class Event(db.Model):
    __tablename__ = 'events'

    # ... existing columns ...

    # Optimistic locking
    version = db.Column(db.Integer, nullable=False, default=0)

    __mapper_args__ = {
        "version_id_col": version
    }
```

3. **Create atomic capacity checker with row locking**

In `app/services/registrations/registration_service.py`, add helper:

```python
from sqlalchemy import func, select
from sqlalchemy.exc import StaleDataError

def _check_and_reserve_capacity(event_id: int, required_spots: int) -> tuple[bool, str | None]:
    """
    Atomically check capacity and reserve spots using row-level locking.

    Returns:
        (success, error_message)
    """
    logger.info(f"Checking capacity: event_id={event_id}, required_spots={required_spots}")

    # Use SELECT FOR UPDATE to lock the event row
    event = db.session.query(Event).filter(
        Event.id == event_id
    ).with_for_update().first()

    if not event:
        return (False, f"Event not found: ID {event_id}")

    # Count current registrations while holding lock
    current_count = db.session.query(func.count(EventParticipant.id)).filter(
        EventParticipant.event_id == event_id,
        EventParticipant.status.in_(['registered', 'confirmed'])
    ).scalar()

    available = event.capacity - current_count

    if required_spots > available:
        logger.warning(
            f"Insufficient capacity: event_id={event_id}, "
            f"required={required_spots}, available={available}"
        )
        return (False, f"Insufficient capacity. {available} spots available, {required_spots} required.")

    logger.info(f"Capacity check passed: event_id={event_id}, available={available}")
    return (True, None)
```

4. **Refactor move_registrations_to_day with optimistic locking**

In `app/services/registrations/registration_service.py:340-365`, replace with:

```python
def move_registrations_to_day(registration_ids: list[int], target_day_id: int) -> dict:
    """
    Move registrations to another day with atomic capacity checking.

    Uses optimistic locking to prevent race conditions.
    """
    logger.info(
        f"Moving registrations: ids={registration_ids}, "
        f"target_day_id={target_day_id}"
    )

    # Retry loop for optimistic locking conflicts
    max_retries = 3
    for attempt in range(max_retries):
        try:
            # Start transaction
            with db.session.begin_nested():
                # Atomic capacity check with row lock
                success, error = _check_and_reserve_capacity(
                    target_day_id,
                    len(registration_ids)
                )

                if not success:
                    return {'success': False, 'errors': [error]}

                # Fetch registrations to move
                registrations = db.session.query(EventParticipant).filter(
                    EventParticipant.id.in_(registration_ids)
                ).all()

                if len(registrations) != len(registration_ids):
                    return {
                        'success': False,
                        'errors': ['One or more registrations not found']
                    }

                # Move all registrations
                for reg in registrations:
                    old_event_id = reg.event_id
                    reg.event_id = target_day_id
                    logger.info(
                        f"Moved registration: id={reg.id}, "
                        f"from_event={old_event_id}, to_event={target_day_id}"
                    )

            # Commit outer transaction
            db.session.commit()

            logger.info(
                f"Successfully moved {len(registrations)} registrations "
                f"to event {target_day_id}"
            )
            return {'success': True, 'moved_count': len(registrations)}

        except StaleDataError:
            # Optimistic locking conflict - another transaction modified the event
            db.session.rollback()
            logger.warning(
                f"Optimistic locking conflict on attempt {attempt + 1}/{max_retries} "
                f"for event {target_day_id}"
            )

            if attempt == max_retries - 1:
                return {
                    'success': False,
                    'errors': ['Event was modified by another user. Please try again.']
                }

            # Small delay before retry
            import time
            time.sleep(0.1 * (attempt + 1))

        except Exception as e:
            db.session.rollback()
            logger.error(
                f"Error moving registrations: {str(e)}",
                exc_info=True
            )
            return {'success': False, 'errors': [str(e)]}

    # Should not reach here
    return {'success': False, 'errors': ['Unexpected error in retry loop']}
```

5. **Apply same pattern to waiting list promotion**

In `app/services/registrations/waiting_list_service.py:180-220`, refactor `auto_promote_from_waiting_list`:

```python
def auto_promote_from_waiting_list(event_id: int) -> dict:
    """
    Automatically promote participants from waiting list when spots become available.

    Uses row-level locking to prevent race conditions.
    """
    logger.info(f"Auto-promoting from waiting list: event_id={event_id}")

    max_retries = 3
    for attempt in range(max_retries):
        try:
            with db.session.begin_nested():
                # Lock event row and check capacity
                event = db.session.query(Event).filter(
                    Event.id == event_id
                ).with_for_update().first()

                if not event:
                    return {'success': False, 'error': 'Event not found'}

                # Count current registrations
                current_count = db.session.query(func.count(EventParticipant.id)).filter(
                    EventParticipant.event_id == event_id,
                    EventParticipant.status.in_(['registered', 'confirmed'])
                ).scalar()

                available_spots = event.capacity - current_count

                if available_spots <= 0:
                    logger.info(f"No available spots: event_id={event_id}")
                    return {'success': True, 'promoted': 0}

                # Get waiting list entries in order
                waiting = db.session.query(WaitingList).filter(
                    WaitingList.event_id == event_id,
                    WaitingList.status == 'waiting'
                ).order_by(WaitingList.position).limit(available_spots).all()

                promoted_count = 0
                for entry in waiting:
                    # Create registration
                    registration = EventParticipant(
                        event_id=event_id,
                        participant_id=entry.participant_id,
                        status='registered'
                    )
                    db.session.add(registration)

                    # Update waiting list entry
                    entry.status = 'promoted'
                    entry.promoted_at = datetime.now(timezone.utc)

                    promoted_count += 1
                    logger.info(
                        f"Promoted from waiting list: "
                        f"participant_id={entry.participant_id}, event_id={event_id}"
                    )

            db.session.commit()
            logger.info(f"Promoted {promoted_count} participants from waiting list")
            return {'success': True, 'promoted': promoted_count}

        except StaleDataError:
            db.session.rollback()
            logger.warning(
                f"Optimistic locking conflict on attempt {attempt + 1}/{max_retries}"
            )
            if attempt == max_retries - 1:
                return {
                    'success': False,
                    'error': 'Event was modified by another user. Please try again.'
                }
            import time
            time.sleep(0.1 * (attempt + 1))

        except Exception as e:
            db.session.rollback()
            logger.error(f"Error promoting from waiting list: {str(e)}", exc_info=True)
            return {'success': False, 'error': str(e)}
```

**Verification:**

- [ ] Run migration: `flask db upgrade`
- [ ] Create test `tests/test_services/test_race_conditions.py`:

```python
import pytest
import threading
from app.services.registrations.registration_service import move_registrations_to_day
from tests.factories import EventFactory, EventParticipantFactory


def test_concurrent_registration_moves_respect_capacity(db_session):
    """Test that concurrent moves cannot exceed capacity."""
    # Create event with capacity 10, 8 current registrations
    event = EventFactory(capacity=10)
    target_event = EventFactory(capacity=10)

    # Create 8 existing registrations in target
    for _ in range(8):
        EventParticipantFactory(event_id=target_event.id, status='registered')

    # Create 4 registrations to move (would exceed capacity of 10)
    source_regs = [EventParticipantFactory(event_id=event.id).id for _ in range(4)]

    db_session.commit()

    # Split into two concurrent requests
    results = [None, None]

    def move_batch_1():
        results[0] = move_registrations_to_day(source_regs[:2], target_event.id)

    def move_batch_2():
        results[1] = move_registrations_to_day(source_regs[2:], target_event.id)

    # Execute concurrently
    t1 = threading.Thread(target=move_batch_1)
    t2 = threading.Thread(target=move_batch_2)

    t1.start()
    t2.start()
    t1.join()
    t2.join()

    # Exactly one should succeed (2 spots available)
    success_count = sum(1 for r in results if r['success'])
    assert success_count == 1, "Only one batch should succeed due to capacity limit"

    # Verify total registrations = capacity
    final_count = db_session.query(EventParticipant).filter(
        EventParticipant.event_id == target_event.id
    ).count()
    assert final_count == 10, "Should not exceed capacity"
```

- [ ] Run test: `pytest tests/test_services/test_race_conditions.py -v`
- [ ] Verify optimistic locking retry logic by adding logging
- [ ] Test in staging with concurrent requests

---

### Issue 1.2: Deprecated Model.query.get() Pattern

**Problem:** Uses deprecated `Model.query.get()` instead of SQLAlchemy 2.0 recommended `db.session.get(Model, id)`. This violates CLAUDE.md project rules.

**Impact:** Incompatible with SQLAlchemy 2.0, causes session management issues.

**Total Occurrences:** 52 instances across 8 files

**Files:**
- `app/services/analytics/analytics_service.py` (7 instances: lines 386, 813, 882, 977, 1124, 1243, 1388)
- `app/services/analytics/dashboard_service.py` (1 instance: line 969)
- `app/services/events/course_structure_service.py` (12 instances: lines 100, 143, 149, 183, 223, 264, 338, 368, 400, 403, 443, 485)
- `app/services/events/event_template_service.py` (5 instances: lines 109, 245, 358, 737, 796)
- `app/services/feedback/feedback_service.py` (10 instances: lines 467, 501, 508, 599, 628, 637, 724, 793, 873, 949)
- `app/services/feedback/feedback_analytics_service.py` (2 instances: lines 141, 146)
- Additional files with scattered instances

**Implementation Steps:**

1. **Create automated conversion script**

Create `scripts/fix_query_get.py`:

```python
#!/usr/bin/env python3
"""
Script to automatically convert Model.query.get() to db.session.get(Model, id).

Usage:
    python scripts/fix_query_get.py --file app/services/analytics/analytics_service.py
    python scripts/fix_query_get.py --all  # Process all service files
"""

import re
import sys
from pathlib import Path


def convert_query_get(content: str) -> tuple[str, int]:
    """
    Convert Model.query.get(id) to db.session.get(Model, id).

    Returns:
        (converted_content, number_of_replacements)
    """
    # Pattern: ModelName.query.get(id_expression)
    pattern = r'(\w+)\.query\.get\(([^)]+)\)'

    def replace_fn(match):
        model_name = match.group(1)
        id_expr = match.group(2)
        return f'db.session.get({model_name}, {id_expr})'

    converted, count = re.subn(pattern, replace_fn, content)
    return converted, count


def process_file(file_path: Path, dry_run: bool = False) -> dict:
    """Process a single file."""
    content = file_path.read_text(encoding='utf-8')
    converted, count = convert_query_get(content)

    if count > 0:
        if not dry_run:
            file_path.write_text(converted, encoding='utf-8')

        print(f"{'[DRY RUN] ' if dry_run else ''}Converted {count} instances in {file_path}")
        return {'file': str(file_path), 'count': count}

    return None


def main():
    import argparse

    parser = argparse.ArgumentParser(description='Convert Model.query.get() to db.session.get()')
    parser.add_argument('--file', help='Single file to process')
    parser.add_argument('--all', action='store_true', help='Process all service files')
    parser.add_argument('--dry-run', action='store_true', help='Show changes without writing')

    args = parser.parse_args()

    if args.file:
        result = process_file(Path(args.file), args.dry_run)
        if result:
            print(f"\nTotal: {result['count']} replacements")
    elif args.all:
        service_dir = Path('app/services')
        results = []

        for py_file in service_dir.rglob('*.py'):
            result = process_file(py_file, args.dry_run)
            if result:
                results.append(result)

        total = sum(r['count'] for r in results)
        print(f"\n{'[DRY RUN] ' if args.dry_run else ''}Total: {total} replacements across {len(results)} files")
    else:
        parser.print_help()
        sys.exit(1)


if __name__ == '__main__':
    main()
```

2. **Run dry-run to preview changes**

```bash
python scripts/fix_query_get.py --all --dry-run
```

3. **Apply changes to each file**

For `app/services/analytics/analytics_service.py`:

Line 386:
```python
# BEFORE
event = Event.query.get(event_id)

# AFTER
event = db.session.get(Event, event_id)
```

Line 813:
```python
# BEFORE
question = FeedbackQuestion.query.get(question_id)

# AFTER
question = db.session.get(FeedbackQuestion, question_id)
```

Repeat for all 7 instances in the file.

For `app/services/events/course_structure_service.py`:

Lines 100, 143, 149, 183, 223, 264, 338, 368, 400, 403, 443, 485:
```python
# BEFORE
event = Event.query.get(event_id)

# AFTER
event = db.session.get(Event, event_id)
```

4. **Run automated script**

```bash
python scripts/fix_query_get.py --all
```

5. **Manual verification of conversions**

Check each converted file for correctness:
- Ensure model names are capitalized correctly
- Verify ID expressions are preserved exactly
- Check for any complex expressions that need parentheses

**Verification:**

- [ ] Run dry-run: `python scripts/fix_query_get.py --all --dry-run`
- [ ] Review output for expected 52 replacements
- [ ] Run actual conversion: `python scripts/fix_query_get.py --all`
- [ ] Search for remaining instances: `grep -r "\.query\.get(" app/services/`
- [ ] Verify result is empty or only false positives
- [ ] Run full test suite: `pytest tests/ -q`
- [ ] Manually test affected features:
  - [ ] Analytics dashboard loads
  - [ ] Event template copying works
  - [ ] Feedback forms function correctly
  - [ ] Course structure operations work

---

### Issue 1.3: Cache Bug with Mutable Parameters

**Problem:** `@cache.memoize()` decorator doesn't include `filters` dictionary in cache key. Different filter values return stale cached data from the first call.

**Impact:** Users get incorrect analytics data when applying different date ranges or filters.

**Files:**
- `app/services/analytics/analytics_service.py:779`

**Current Problematic Code:**

```python
# Line 779
@cache.memoize(timeout=300)
def get_question_analytics(question_id: int, filters: Optional[dict] = None):
    """Get analytics for a specific question."""
    # Cache key only includes question_id!
    # filters={'date_from': '2025-01-01'} and filters={'date_from': '2024-01-01'}
    # both return the same cached result
```

**Example Bug:**

```python
# First call - cached with key: "get_question_analytics:42"
result1 = get_question_analytics(42, filters={'date_from': '2025-01-01'})

# Second call - returns result1 even though filters are different!
result2 = get_question_analytics(42, filters={'date_from': '2024-01-01'})

# result1 == result2 (WRONG!)
```

**Implementation Steps:**

1. **Create custom cache key function**

Add to `app/services/analytics/analytics_service.py` at the top (after imports):

```python
import hashlib
import json

def _make_cache_key_with_filters(question_id: int, filters: Optional[dict] = None) -> str:
    """
    Create cache key that includes serialized filters.

    Ensures different filter values generate different cache keys.
    """
    # Base key
    key_parts = [f"question_analytics:{question_id}"]

    # Add filters if provided
    if filters:
        # Sort keys for consistent ordering
        sorted_filters = json.dumps(filters, sort_keys=True)
        filter_hash = hashlib.md5(sorted_filters.encode()).hexdigest()
        key_parts.append(f"filters:{filter_hash}")
    else:
        key_parts.append("filters:none")

    return ":".join(key_parts)
```

2. **Update get_question_analytics to use custom cache key**

Replace line 779-780:

```python
# BEFORE
@cache.memoize(timeout=300)
def get_question_analytics(question_id: int, filters: Optional[dict] = None):

# AFTER
def get_question_analytics(question_id: int, filters: Optional[dict] = None):
    """
    Get analytics for a specific question.

    Args:
        question_id: ID of the feedback question
        filters: Optional dict with keys: date_from, date_to, event_ids

    Returns:
        Dict with analytics data including response counts and statistics
    """
    # Manual cache management with proper key
    cache_key = _make_cache_key_with_filters(question_id, filters)

    # Try to get from cache
    cached_result = cache.get(cache_key)
    if cached_result is not None:
        logger.info(f"Cache hit: {cache_key}")
        return cached_result

    logger.info(f"Cache miss: {cache_key}")

    # Compute analytics (existing logic below)
    result = _compute_question_analytics(question_id, filters)

    # Store in cache
    cache.set(cache_key, result, timeout=300)

    return result
```

3. **Extract computation logic to separate function**

Add new function below get_question_analytics:

```python
def _compute_question_analytics(question_id: int, filters: Optional[dict] = None) -> dict:
    """
    Compute question analytics (extracted for testability).

    This is the actual computation logic, separated from caching.
    """
    question = db.session.get(FeedbackQuestion, question_id)
    if not question:
        raise ValueError(f"Question not found: {question_id}")

    # Build base query
    query = db.session.query(FeedbackResponse).filter(
        FeedbackResponse.question_id == question_id
    )

    # Apply filters if provided
    if filters:
        if 'date_from' in filters:
            date_from = filters['date_from']
            if isinstance(date_from, str):
                date_from = datetime.fromisoformat(date_from)
            query = query.filter(FeedbackResponse.created_at >= date_from)

        if 'date_to' in filters:
            date_to = filters['date_to']
            if isinstance(date_to, str):
                date_to = datetime.fromisoformat(date_to)
            query = query.filter(FeedbackResponse.created_at <= date_to)

        if 'event_ids' in filters:
            query = query.filter(FeedbackResponse.event_id.in_(filters['event_ids']))

    # Get all responses
    responses = query.all()

    # Compute statistics (existing logic from original function)
    # ... rest of existing computation ...

    return {
        'question_id': question_id,
        'total_responses': len(responses),
        'statistics': statistics,
        'filters_applied': filters or {}
    }
```

4. **Update cache invalidation functions**

Find and update the cache invalidation in `analytics_service.py:1719-1757`:

```python
def invalidate_question_analytics(question_id: int):
    """
    Invalidate all cached analytics for a question.

    Must clear all filter variations.
    """
    # Since we don't know all possible filter combinations,
    # we need to clear by pattern
    # This requires redis cache backend with key pattern matching

    pattern = f"question_analytics:{question_id}:*"

    # If using redis
    if hasattr(cache.cache, 'delete_many'):
        keys = cache.cache._client.keys(pattern)
        if keys:
            cache.cache.delete_many(*keys)
            logger.info(f"Invalidated {len(keys)} cache keys for question {question_id}")
    else:
        # Fallback: clear entire cache (not ideal but safe)
        logger.warning("Cache backend doesn't support pattern matching, clearing all")
        cache.clear()
```

5. **Add comprehensive tests**

Create `tests/test_services/analytics/test_analytics_cache.py`:

```python
"""Tests for analytics caching behavior."""

import pytest
from datetime import datetime, timedelta
from app.services.analytics.analytics_service import get_question_analytics
from tests.factories import FeedbackQuestionFactory, FeedbackResponseFactory


def test_cache_respects_different_date_filters(db_session, app):
    """Different date filters should return different results."""
    question = FeedbackQuestionFactory()

    # Create responses in different time periods
    old_date = datetime(2024, 1, 1)
    new_date = datetime(2025, 1, 1)

    FeedbackResponseFactory(
        question_id=question.id,
        created_at=old_date,
        response_text="Old response"
    )
    FeedbackResponseFactory(
        question_id=question.id,
        created_at=new_date,
        response_text="New response"
    )

    db_session.commit()

    # Test with old filter
    result_old = get_question_analytics(
        question.id,
        filters={'date_from': '2024-01-01', 'date_to': '2024-12-31'}
    )

    # Test with new filter
    result_new = get_question_analytics(
        question.id,
        filters={'date_from': '2025-01-01', 'date_to': '2025-12-31'}
    )

    # Should be different!
    assert result_old['total_responses'] == 1
    assert result_new['total_responses'] == 1
    assert result_old != result_new


def test_cache_hit_with_identical_filters(db_session, app):
    """Identical filters should use cache."""
    question = FeedbackQuestionFactory()
    FeedbackResponseFactory(question_id=question.id)
    db_session.commit()

    filters = {'date_from': '2025-01-01'}

    # First call
    result1 = get_question_analytics(question.id, filters=filters)

    # Second call should hit cache (verify with logging)
    result2 = get_question_analytics(question.id, filters=filters)

    assert result1 == result2


def test_cache_none_filters_vs_empty_dict(db_session, app):
    """None filters and {} should be treated as same."""
    question = FeedbackQuestionFactory()
    FeedbackResponseFactory(question_id=question.id)
    db_session.commit()

    result_none = get_question_analytics(question.id, filters=None)
    result_empty = get_question_analytics(question.id, filters={})

    # Should return same data
    assert result_none['total_responses'] == result_empty['total_responses']
```

**Verification:**

- [ ] Run tests: `pytest tests/test_services/analytics/test_analytics_cache.py -v`
- [ ] All tests pass
- [ ] Manual test in UI:
  - [ ] Open feedback analytics page
  - [ ] Apply date filter "2024-01-01 to 2024-12-31"
  - [ ] Note response count
  - [ ] Change filter to "2025-01-01 to 2025-12-31"
  - [ ] Verify response count changes (previously would stay same)
- [ ] Check logs for cache hits/misses
- [ ] Verify cache keys in Redis (if using redis):
  ```bash
  redis-cli KEYS "question_analytics:*"
  ```

---

### Issue 1.4: Missing Database Transactions in Event Type Service

**Problem:** Event type operations flush data before audit logging. If audit logging fails, partial state persists in database.

**Impact:** Inconsistent data state, audit log gaps, potential regulatory compliance issues.

**Files:**
- `app/services/events/event_type_service.py` (lines 418-437, 470-530, 540-578)

**Current Problematic Code:**

```python
# Lines 418-437 (create_event_type)
def create_event_type(code: str, label_en: str, user_id: int) -> EventType:
    event_type = EventType(code=code, label_en=label_en)
    db.session.add(event_type)
    db.session.flush()  # Data persisted to DB

    # If this fails, event_type still exists!
    log_settings_change(
        user_id=user_id,
        table_name='event_types',
        record_id=event_type.id,
        action='create'
    )

    db.session.commit()
    return event_type
```

**Implementation Steps:**

1. **Refactor create_event_type with nested transaction**

In `app/services/events/event_type_service.py`, replace lines 418-437:

```python
def create_event_type(code: str, label_en: str, label_fr: str, label_nl: str, user_id: int) -> EventType:
    """
    Create a new event type with atomic audit logging.

    Uses nested transactions to ensure audit log is created with the event type.
    """
    logger.info(f"Creating event type: code='{code}', user_id={user_id}")

    # Validation
    if not code or not code.strip():
        logger.warning("Event type creation failed: code is required")
        raise ValueError(ERR_EVENT_TYPE_CODE_REQUIRED)

    # Check for duplicate code
    existing = db.session.query(EventType).filter(
        EventType.code == code.strip().upper()
    ).first()

    if existing:
        logger.warning(f"Event type creation failed: duplicate code '{code}'")
        raise ValueError(f"Event type code '{code}' already exists.")

    try:
        # Use nested transaction for atomicity
        with db.session.begin_nested():
            # Create event type
            event_type = EventType(
                code=code.strip().upper(),
                label_en=label_en.strip(),
                label_fr=label_fr.strip(),
                label_nl=label_nl.strip()
            )
            db.session.add(event_type)
            db.session.flush()  # Get ID for audit log

            # Create audit log (part of same transaction)
            log_settings_change(
                user_id=user_id,
                table_name='event_types',
                record_id=event_type.id,
                action='create',
                new_value=event_type.code
            )

        # Commit outer transaction (both event type AND audit log)
        db.session.commit()

        # Invalidate cache
        invalidate_event_types_cache()

        logger.info(f"Event type created successfully: id={event_type.id}, code='{code}'")
        return event_type

    except ValueError:
        # Re-raise validation errors
        raise
    except Exception as e:
        db.session.rollback()
        logger.error(f"Failed to create event type: {str(e)}", exc_info=True)
        raise ValueError(f"Failed to create event type: {str(e)}")
```

2. **Refactor update_event_type with nested transaction**

In `app/services/events/event_type_service.py`, replace lines 470-530:

```python
def update_event_type(event_type_id: int, data: dict, user_id: int) -> EventType:
    """
    Update an event type with atomic audit logging.

    Args:
        event_type_id: ID of event type to update
        data: Dict with keys: code, label_en, label_fr, label_nl
        user_id: ID of user making the change

    Returns:
        Updated EventType instance
    """
    logger.info(f"Updating event type: id={event_type_id}, fields={list(data.keys())}")

    # Get existing event type
    event_type = db.session.get(EventType, event_type_id)
    if not event_type:
        logger.warning(f"Event type not found: id={event_type_id}")
        raise ValueError(ERR_EVENT_TYPE_NOT_FOUND)

    # Validate code uniqueness if changed
    if 'code' in data and data['code'] != event_type.code:
        new_code = data['code'].strip().upper()
        existing = db.session.query(EventType).filter(
            EventType.code == new_code,
            EventType.id != event_type_id
        ).first()

        if existing:
            logger.warning(f"Event type update failed: duplicate code '{new_code}'")
            raise ValueError(f"Event type code '{new_code}' already exists.")

    try:
        with db.session.begin_nested():
            # Track changes for audit log
            changes = {}

            # Update fields
            for field in ['code', 'label_en', 'label_fr', 'label_nl']:
                if field in data:
                    old_value = getattr(event_type, field)
                    new_value = data[field].strip()

                    if field == 'code':
                        new_value = new_value.upper()

                    if old_value != new_value:
                        changes[field] = {'old': old_value, 'new': new_value}
                        setattr(event_type, field, new_value)

            db.session.flush()

            # Log each changed field
            for field, values in changes.items():
                log_settings_change(
                    user_id=user_id,
                    table_name='event_types',
                    record_id=event_type_id,
                    action='update',
                    field_name=field,
                    old_value=values['old'],
                    new_value=values['new']
                )

        # Commit outer transaction
        db.session.commit()

        # Invalidate cache
        invalidate_event_types_cache()

        logger.info(
            f"Event type updated successfully: id={event_type_id}, "
            f"changed_fields={list(changes.keys())}"
        )
        return event_type

    except ValueError:
        raise
    except Exception as e:
        db.session.rollback()
        logger.error(f"Failed to update event type: {str(e)}", exc_info=True)
        raise ValueError(f"Failed to update event type: {str(e)}")
```

3. **Refactor delete_event_type with nested transaction**

In `app/services/events/event_type_service.py`, replace lines 540-578:

```python
def delete_event_type(event_type_id: int, user_id: int) -> None:
    """
    Soft-delete an event type with atomic audit logging.

    Prevents deletion if event type is in use by any events.
    """
    logger.info(f"Deleting event type: id={event_type_id}, user_id={user_id}")

    # Get event type
    event_type = db.session.get(EventType, event_type_id)
    if not event_type:
        logger.warning(f"Event type not found: id={event_type_id}")
        raise ValueError(ERR_EVENT_TYPE_NOT_FOUND)

    # Check if in use
    events_count = db.session.query(func.count(Event.id)).filter(
        Event.event_type_id == event_type_id,
        Event.deleted_at.is_(None)
    ).scalar()

    if events_count > 0:
        logger.warning(
            f"Event type deletion blocked: {events_count} events use type {event_type_id}"
        )
        raise ValueError(
            f"Cannot delete event type '{event_type.code}'. "
            f"It is used by {events_count} event(s)."
        )

    try:
        with db.session.begin_nested():
            # Soft delete
            old_code = event_type.code
            event_type.deleted_at = datetime.now(timezone.utc)
            db.session.flush()

            # Audit log
            log_settings_change(
                user_id=user_id,
                table_name='event_types',
                record_id=event_type_id,
                action='delete',
                old_value=old_code
            )

        # Commit outer transaction
        db.session.commit()

        # Invalidate cache
        invalidate_event_types_cache()

        logger.info(f"Event type deleted successfully: id={event_type_id}, code='{old_code}'")

    except ValueError:
        raise
    except Exception as e:
        db.session.rollback()
        logger.error(f"Failed to delete event type: {str(e)}", exc_info=True)
        raise ValueError(f"Failed to delete event type: {str(e)}")
```

4. **Add tests for transaction atomicity**

Create `tests/test_services/events/test_event_type_transactions.py`:

```python
"""Tests for event type service transaction atomicity."""

import pytest
from unittest.mock import patch
from app.services.events.event_type_service import (
    create_event_type,
    update_event_type,
    delete_event_type
)
from app.models.events.event_type import EventType
from tests.factories import UserFactory


def test_create_event_type_rolls_back_on_audit_failure(db_session):
    """If audit logging fails, event type should not be created."""
    user = UserFactory()
    db_session.commit()

    # Mock audit logging to raise exception
    with patch('app.services.events.event_type_service.log_settings_change') as mock_log:
        mock_log.side_effect = Exception("Audit log failed")

        # Attempt to create event type
        with pytest.raises(ValueError, match="Failed to create event type"):
            create_event_type(
                code='TEST',
                label_en='Test Type',
                label_fr='Type Test',
                label_nl='Test Type',
                user_id=user.id
            )

    # Verify event type was NOT created (transaction rolled back)
    event_type = db_session.query(EventType).filter(EventType.code == 'TEST').first()
    assert event_type is None, "Event type should not exist after rollback"


def test_update_event_type_rolls_back_on_audit_failure(db_session):
    """If audit logging fails, event type update should be rolled back."""
    user = UserFactory()

    # Create event type
    event_type = create_event_type(
        code='TEST',
        label_en='Original',
        label_fr='Original',
        label_nl='Original',
        user_id=user.id
    )
    db_session.commit()

    original_label = event_type.label_en

    # Mock audit logging to fail
    with patch('app.services.events.event_type_service.log_settings_change') as mock_log:
        mock_log.side_effect = Exception("Audit log failed")

        # Attempt to update
        with pytest.raises(ValueError, match="Failed to update event type"):
            update_event_type(
                event_type.id,
                {'label_en': 'Modified'},
                user_id=user.id
            )

    # Refresh from database
    db_session.refresh(event_type)

    # Verify label was NOT changed (transaction rolled back)
    assert event_type.label_en == original_label
```

**Verification:**

- [ ] Run tests: `pytest tests/test_services/events/test_event_type_transactions.py -v`
- [ ] All tests pass
- [ ] Manual test with breakpoint in audit logging:
  - [ ] Add `import pdb; pdb.set_trace()` in log_settings_change
  - [ ] Create event type via UI
  - [ ] When debugger breaks, raise exception
  - [ ] Verify event type not in database
- [ ] Test successful path:
  - [ ] Create event type via UI
  - [ ] Verify both event type AND audit log exist
  - [ ] Update event type
  - [ ] Verify update audit log exists
  - [ ] Delete event type
  - [ ] Verify delete audit log exists

---

### Issue 1.5: Dead Code Files

**Problem:** Two service files contain no actual implementation and are never imported anywhere in the codebase.

**Impact:** Confuses developers, adds maintenance burden, suggests incomplete features.

**Files:**
- `app/services/documents/certificate_generator.py` (13 lines, placeholder stub)
- `app/services/speakers/speaker_service.py` (1 line, empty file)

**Implementation Steps:**

1. **Verify files are not imported**

```bash
# Search for imports of certificate_generator
grep -r "from app.services.documents.certificate_generator" app/
grep -r "import certificate_generator" app/

# Search for imports of speaker_service
grep -r "from app.services.speakers.speaker_service" app/
grep -r "import speaker_service" app/
```

Expected: No results (files are never imported)

2. **Check git history for context**

```bash
git log --oneline app/services/documents/certificate_generator.py
git log --oneline app/services/speakers/speaker_service.py
```

3. **Delete the files**

```bash
rm app/services/documents/certificate_generator.py
rm app/services/speakers/speaker_service.py
```

4. **Verify no broken imports**

```bash
# Run Python import check
python -c "from app import create_app; app = create_app()"

# Run tests
pytest tests/ -q
```

**Verification:**

- [ ] Confirm files not imported: `grep -r "certificate_generator\|speaker_service" app/`
- [ ] Delete files
- [ ] Run app: `python -c "from app import create_app; app = create_app()"`
- [ ] Run tests: `pytest tests/ -q`
- [ ] All tests pass
- [ ] No import errors

---

## Phase 2: Critical Code Fixes

**Status: COMPLETED** (2026-01-18)

All Phase 2 issues have been addressed:
- Issue 2.1: Added proper exports to all services __init__.py files
- Issue 2.2: Deleted 3 dead functions from event_linking_service.py
- Issue 2.3: Fixed N+1 query in analytics_service.py (get_trends_analytics) using joinedload
- Issue 2.4: Fixed N+1 query in compare_speakers using batch loading with .in_()
- Issue 2.5: Fixed N+1 query in generate_event_report using subquery
- Issue 2.6: Fixed N+1 query in generate_participant_report using GROUP BY aggregation
- Issue 2.7: Verified - calculate_revenue function doesn't exist; existing code already has proper eager loading
- Issue 2.8: Fixed N+1 query in copy_event_with_children using joinedload for child relationships

---

These issues cause incorrect behavior or violate coding standards. They should be fixed immediately after Phase 1.

### Issue 2.1: Missing __init__.py Exports (Breaking Backwards Compatibility)

**Problem:** Service subdirectory `__init__.py` files are empty, forcing imports to use full paths. Refactoring breaks existing imports.

**Impact:** When services are split or reorganized, all import paths break across routes and other services.

**Files with Missing Exports:**
- `app/services/events/__init__.py`
- `app/services/tasks/__init__.py`
- `app/services/registrations/__init__.py`
- `app/services/shared/__init__.py`
- `app/services/users/__init__.py`
- `app/services/finance/__init__.py`
- `app/services/speakers/__init__.py`

**Missing Exports in Main __init__.py:**
- `copy_event_with_children` (used in routes/events.py)
- `auto_promote_from_waiting_list` (used in routes/api/registrations)

**Implementation Steps:**

1. **Update app/services/events/__init__.py**

Replace empty file with:

```python
"""Event management services."""

# Event template operations
from app.services.events.event_template_service import (
    get_all_templates,
    create_template,
    update_template,
    delete_template,
    create_event_from_template,
    copy_event,
    copy_event_with_children,
)

# Event linking operations
from app.services.events.event_linking_service import (
    link_events,
    unlink_events,
    get_linked_events,
    get_edition_sequence,
)

# Course structure operations
from app.services.events.course_structure_service import (
    CourseStructureService,
)

# Event type operations
from app.services.events.event_type_service import (
    get_all_event_types,
    create_event_type,
    update_event_type,
    delete_event_type,
)

# Event notification operations
from app.services.events.event_notification_service import (
    send_event_notification,
    get_notification_recipients,
)

__all__ = [
    # Templates
    'get_all_templates',
    'create_template',
    'update_template',
    'delete_template',
    'create_event_from_template',
    'copy_event',
    'copy_event_with_children',
    # Linking
    'link_events',
    'unlink_events',
    'get_linked_events',
    'get_edition_sequence',
    # Course structure
    'CourseStructureService',
    # Event types
    'get_all_event_types',
    'create_event_type',
    'update_event_type',
    'delete_event_type',
    # Notifications
    'send_event_notification',
    'get_notification_recipients',
]
```

2. **Update app/services/tasks/__init__.py**

```python
"""Task management services."""

# Task operations
from app.services.tasks.task_service import (
    get_all_tasks,
    create_task,
    update_task,
    delete_task,
    get_tasks_for_event,
    get_tasks_for_user,
)

# Action item operations
from app.services.tasks.action_item_service import (
    create_action_item,
    update_action_item,
    mark_action_item_complete,
    get_action_items_for_event,
)

# Recurring task operations
from app.services.tasks.recurring_task_service import (
    create_recurring_task,
    generate_next_occurrence,
    get_recurring_tasks,
)

# Task template operations
from app.services.tasks.task_template_service import (
    get_task_templates,
    create_task_template,
    apply_task_template,
)

__all__ = [
    # Tasks
    'get_all_tasks',
    'create_task',
    'update_task',
    'delete_task',
    'get_tasks_for_event',
    'get_tasks_for_user',
    # Action items
    'create_action_item',
    'update_action_item',
    'mark_action_item_complete',
    'get_action_items_for_event',
    # Recurring tasks
    'create_recurring_task',
    'generate_next_occurrence',
    'get_recurring_tasks',
    # Templates
    'get_task_templates',
    'create_task_template',
    'apply_task_template',
]
```

3. **Update app/services/registrations/__init__.py**

```python
"""Registration and waiting list services."""

# Registration operations
from app.services.registrations.registration_service import (
    create_registration,
    update_registration,
    cancel_registration,
    move_registrations_to_day,
    get_registrations_for_event,
)

# Waiting list operations
from app.services.registrations.waiting_list_service import (
    add_to_waiting_list,
    remove_from_waiting_list,
    auto_promote_from_waiting_list,
    get_waiting_list_for_event,
    reorder_waiting_list,
)

__all__ = [
    # Registrations
    'create_registration',
    'update_registration',
    'cancel_registration',
    'move_registrations_to_day',
    'get_registrations_for_event',
    # Waiting list
    'add_to_waiting_list',
    'remove_from_waiting_list',
    'auto_promote_from_waiting_list',
    'get_waiting_list_for_event',
    'reorder_waiting_list',
]
```

4. **Update app/services/shared/__init__.py**

```python
"""Shared services used across the application."""

# Email services
from app.services.shared.email_service import (
    send_email,
    send_bulk_email,
)

# Calendar services
from app.services.shared.calendar_service import (
    generate_ics_file,
    get_calendar_events,
)

# Settings services
from app.services.shared.settings_service import (
    get_setting,
    update_setting,
    get_all_settings,
)

# Audit services
from app.services.shared.audit_service import (
    log_settings_change,
    get_audit_log,
)

__all__ = [
    # Email
    'send_email',
    'send_bulk_email',
    # Calendar
    'generate_ics_file',
    'get_calendar_events',
    # Settings
    'get_setting',
    'update_setting',
    'get_all_settings',
    # Audit
    'log_settings_change',
    'get_audit_log',
]
```

5. **Update app/services/users/__init__.py**

```python
"""User-related services."""

# Notification services
from app.services.users.notification_service import (
    create_notification,
    mark_notification_read,
    get_unread_notifications,
    delete_notification,
)

# User preference services
from app.services.users.user_preference_service import (
    get_user_preferences,
    update_user_preferences,
)

__all__ = [
    # Notifications
    'create_notification',
    'mark_notification_read',
    'get_unread_notifications',
    'delete_notification',
    # Preferences
    'get_user_preferences',
    'update_user_preferences',
]
```

6. **Update app/services/finance/__init__.py**

```python
"""Financial services for pricing and invoicing."""

# Pricing services
from app.services.finance.pricing_service import (
    calculate_price,
    get_pricing_rules,
    create_pricing_rule,
    update_pricing_rule,
)

# Financial services
from app.services.finance.financial_service import (
    get_financial_summary,
    calculate_revenue,
    get_payment_status,
)

# Carryover services
from app.services.finance.carryover_service import (
    process_carryovers,
    get_carryover_balance,
)

__all__ = [
    # Pricing
    'calculate_price',
    'get_pricing_rules',
    'create_pricing_rule',
    'update_pricing_rule',
    # Financial
    'get_financial_summary',
    'calculate_revenue',
    'get_payment_status',
    # Carryover
    'process_carryovers',
    'get_carryover_balance',
]
```

7. **Update app/services/speakers/__init__.py**

```python
"""Speaker-related services."""

# Speaker photo services
from app.services.speakers.speaker_photo_service import (
    upload_speaker_photo,
    delete_speaker_photo,
    get_speaker_photo_url,
)

# Speaker title services
from app.services.speakers.speaker_title_service import (
    get_all_speaker_titles,
    create_speaker_title,
    update_speaker_title,
    delete_speaker_title,
)

__all__ = [
    # Photos
    'upload_speaker_photo',
    'delete_speaker_photo',
    'get_speaker_photo_url',
    # Titles
    'get_all_speaker_titles',
    'create_speaker_title',
    'update_speaker_title',
    'delete_speaker_title',
]
```

8. **Update main app/services/__init__.py**

Add missing top-level exports:

```python
"""Service layer for business logic."""

# Re-export commonly used services for convenience
from app.services.events import copy_event_with_children
from app.services.registrations import auto_promote_from_waiting_list

__all__ = [
    'copy_event_with_children',
    'auto_promote_from_waiting_list',
]
```

9. **Update imports in routes to use shorter paths**

In `app/routes/events.py`, change:

```python
# BEFORE
from app.services.events.event_template_service import copy_event_with_children

# AFTER (both work, shorter is preferred)
from app.services.events import copy_event_with_children
# OR
from app.services import copy_event_with_children
```

**Verification:**

- [ ] Run Python to check imports:
  ```python
  python -c "from app.services.events import copy_event_with_children; print('OK')"
  python -c "from app.services import auto_promote_from_waiting_list; print('OK')"
  ```
- [ ] Run full test suite: `pytest tests/ -q`
- [ ] Check app starts: `flask run`
- [ ] Create test script `scripts/verify_imports.py`:

```python
"""Verify all service exports work."""

import sys

# Test all exports
try:
    from app.services.events import (
        create_template, copy_event_with_children
    )
    print("[OK] events exports")

    from app.services.tasks import (
        create_task, get_all_tasks
    )
    print("[OK] tasks exports")

    from app.services.registrations import (
        auto_promote_from_waiting_list
    )
    print("[OK] registrations exports")

    from app.services.shared import (
        send_email, log_settings_change
    )
    print("[OK] shared exports")

    from app.services.users import (
        create_notification
    )
    print("[OK] users exports")

    from app.services.finance import (
        calculate_price
    )
    print("[OK] finance exports")

    from app.services.speakers import (
        upload_speaker_photo
    )
    print("[OK] speakers exports")

    from app.services import (
        copy_event_with_children,
        auto_promote_from_waiting_list
    )
    print("[OK] top-level exports")

    print("\nAll imports verified successfully!")
    sys.exit(0)

except ImportError as e:
    print(f"[FAIL] Import error: {e}")
    sys.exit(1)
```

- [ ] Run: `python scripts/verify_imports.py`

---

### Issue 2.2: Dead Functions (Unused Code)

**Problem:** Five functions are defined but never called anywhere in the codebase. They add confusion and maintenance burden.

**Impact:** Code bloat, confusion about whether features are implemented, wasted maintenance effort.

**Functions to Delete:**
1. `suggest_previous_edition()` - `event_linking_service.py:216`
2. `get_analytics_group()` - `event_linking_service.py:348`
3. `event_link_to_dict()` - `event_linking_service.py:387`

**Functions to Keep (Future Use):**
1. `merge_membership_types()` - `membership_type_service.py:413` (future bulk operation)
2. `split_membership_type()` - `membership_type_service.py:479` (future data correction)

**Implementation Steps:**

1. **Verify functions are not called**

```bash
# Search for each function call
grep -r "suggest_previous_edition" app/ tests/
grep -r "get_analytics_group" app/ tests/
grep -r "event_link_to_dict" app/ tests/
```

Expected: Only definition, no calls

2. **Check frontend JavaScript for calls**

```bash
grep -r "suggest_previous_edition" app/static/js/
grep -r "get_analytics_group" app/static/js/
grep -r "event_link_to_dict" app/static/js/
```

Expected: No results

3. **Delete suggest_previous_edition()**

In `app/services/events/event_linking_service.py`, remove lines 216-267:

```python
# DELETE THIS ENTIRE FUNCTION
def suggest_previous_edition(event_id: int) -> list[Event]:
    """
    Suggest potential previous editions for an event based on title similarity.

    Returns list of events ordered by similarity score.
    """
    # ... 50 lines of code ...
```

4. **Delete get_analytics_group()**

In `app/services/events/event_linking_service.py`, remove lines 348-385:

```python
# DELETE THIS ENTIRE FUNCTION
def get_analytics_group(event_id: int) -> list[Event]:
    """
    Get all events in the same analytics group.

    Traverses the link graph to find all connected events.
    """
    # ... 35 lines of code ...
```

5. **Delete event_link_to_dict()**

In `app/services/events/event_linking_service.py`, remove lines 387-410:

```python
# DELETE THIS ENTIRE FUNCTION
def event_link_to_dict(link: EventLink) -> dict:
    """
    Convert EventLink to dictionary representation.

    Returns dict with link details and related event info.
    """
    # ... 20 lines of code ...
```

6. **Add comments to preserved functions**

In `app/services/settings/membership_type_service.py`, add documentation:

```python
# Line 413
def merge_membership_types(source_id: int, target_id: int, user_id: int) -> dict:
    """
    Merge two membership types by moving all participants from source to target.

    NOTE: Currently unused. Reserved for future bulk data cleanup operations.

    Args:
        source_id: Membership type to merge from (will be soft-deleted)
        target_id: Membership type to merge into
        user_id: User performing the operation

    Returns:
        Dict with success status and count of moved participants
    """
    # ... existing implementation ...


# Line 479
def split_membership_type(membership_type_id: int, split_criteria: dict, user_id: int) -> dict:
    """
    Split a membership type into two based on criteria.

    NOTE: Currently unused. Reserved for future data correction operations.

    Args:
        membership_type_id: Membership type to split
        split_criteria: Dict specifying how to split (e.g., by date, company)
        user_id: User performing the operation

    Returns:
        Dict with success status and IDs of old and new membership types
    """
    # ... existing implementation ...
```

**Verification:**

- [ ] Verify no calls exist:
  ```bash
  grep -r "suggest_previous_edition\|get_analytics_group\|event_link_to_dict" app/ tests/ app/static/js/
  ```
- [ ] Delete the three functions
- [ ] Run tests: `pytest tests/ -q`
- [ ] Start app: `flask run`
- [ ] Check that event linking features still work:
  - [ ] Open event detail page
  - [ ] Link event to another event
  - [ ] View linked events
  - [ ] Unlink events

---

### Issue 2.3: N+1 Query in analytics_service.py (get_question_analytics)

**Problem:** Loops through responses and loads Event for each one individually instead of using a join.

**Impact:** For 100 responses, executes 101 queries instead of 2. Severely degrades analytics performance.

**File:** `app/services/analytics/analytics_service.py:1410`

**Current Problematic Code:**

```python
def get_question_analytics(question_id: int, filters: Optional[dict] = None):
    # ... get responses ...

    responses = query.all()  # Query 1: Get all responses

    # N+1 query loop
    for response in responses:
        event = Event.query.get(response.event_id)  # Query 2, 3, 4, ... N+1
        # Use event data
```

**Implementation Steps:**

1. **Refactor with eager loading**

In `app/services/analytics/analytics_service.py`, find the `_compute_question_analytics` function and update:

```python
def _compute_question_analytics(question_id: int, filters: Optional[dict] = None) -> dict:
    """Compute question analytics with optimized query."""
    from sqlalchemy.orm import joinedload

    question = db.session.get(FeedbackQuestion, question_id)
    if not question:
        raise ValueError(f"Question not found: {question_id}")

    # Build query with eager loading
    query = db.session.query(FeedbackResponse).options(
        joinedload(FeedbackResponse.event)  # Eager load event relationship
    ).filter(
        FeedbackResponse.question_id == question_id
    )

    # Apply filters
    if filters:
        if 'date_from' in filters:
            date_from = filters['date_from']
            if isinstance(date_from, str):
                date_from = datetime.fromisoformat(date_from)
            query = query.filter(FeedbackResponse.created_at >= date_from)

        if 'date_to' in filters:
            date_to = filters['date_to']
            if isinstance(date_to, str):
                date_to = datetime.fromisoformat(date_to)
            query = query.filter(FeedbackResponse.created_at <= date_to)

        if 'event_ids' in filters:
            query = query.filter(FeedbackResponse.event_id.in_(filters['event_ids']))

    # Execute query (single query loads responses + events)
    responses = query.all()

    logger.info(f"Loaded {len(responses)} responses for question {question_id}")

    # Process responses - event is already loaded
    event_breakdown = {}
    for response in responses:
        # No additional query - event is already loaded!
        event_id = response.event_id
        event_title = response.event.title if response.event else 'Unknown'

        if event_id not in event_breakdown:
            event_breakdown[event_id] = {
                'event_id': event_id,
                'event_title': event_title,
                'response_count': 0,
                'responses': []
            }

        event_breakdown[event_id]['response_count'] += 1
        event_breakdown[event_id]['responses'].append({
            'id': response.id,
            'response_value': response.response_value,
            'created_at': response.created_at.isoformat()
        })

    return {
        'question_id': question_id,
        'question_text': question.question_text,
        'total_responses': len(responses),
        'event_breakdown': list(event_breakdown.values()),
        'filters_applied': filters or {}
    }
```

**Verification:**

- [ ] Add test with query counting in `tests/test_services/analytics/test_question_analytics_performance.py`:

```python
"""Test query performance for question analytics."""

import pytest
from tests.helpers.query_counter import assert_query_count, track_queries
from tests.factories import (
    FeedbackQuestionFactory,
    FeedbackResponseFactory,
    EventFactory
)
from app.services.analytics.analytics_service import get_question_analytics


def test_question_analytics_no_n_plus_one(db_session, app):
    """Question analytics should not have N+1 queries."""

    # Setup: Create question with 50 responses across 5 events
    question = FeedbackQuestionFactory()
    events = [EventFactory() for _ in range(5)]

    for event in events:
        for _ in range(10):
            FeedbackResponseFactory(
                question_id=question.id,
                event_id=event.id,
                response_value="Test response"
            )

    db_session.commit()

    # Should complete in ~5 queries regardless of response count
    # 1. Get question
    # 2. Get responses with joined events
    # 3-5. Misc metadata queries
    with assert_query_count(10):
        result = get_question_analytics(question.id)

    assert result['total_responses'] == 50
    assert len(result['event_breakdown']) == 5


def test_query_count_scales_with_events_not_responses(db_session, app):
    """Query count should not increase with response count."""

    question = FeedbackQuestionFactory()
    event = EventFactory()
    db_session.commit()

    # Test with 10 responses
    for _ in range(10):
        FeedbackResponseFactory(question_id=question.id, event_id=event.id)
    db_session.commit()

    with track_queries() as queries_10:
        get_question_analytics(question.id)

    # Add 90 more responses (100 total)
    for _ in range(90):
        FeedbackResponseFactory(question_id=question.id, event_id=event.id)
    db_session.commit()

    with track_queries() as queries_100:
        get_question_analytics(question.id)

    # Query count should be the same
    assert len(queries_10) == len(queries_100), (
        f"Query count increased from {len(queries_10)} to {len(queries_100)} "
        "when adding more responses - possible N+1 query"
    )
```

- [ ] Run test: `pytest tests/test_services/analytics/test_question_analytics_performance.py -v`
- [ ] Manual test in UI:
  - [ ] Create event with feedback form
  - [ ] Add 50 responses
  - [ ] Open feedback analytics page
  - [ ] Enable Flask SQL logging in config
  - [ ] Refresh page and check logs for query count
  - [ ] Should see 1-2 queries, not 50+

---

### Issue 2.4: N+1 Query in analytics_service.py (compare_speakers)

**Problem:** Loops through speaker IDs and loads Speaker for each one individually.

**Impact:** For comparing 10 speakers, executes 11 queries instead of 1.

**File:** `app/services/analytics/analytics_service.py:1243`

**Current Problematic Code:**

```python
def compare_speakers(speaker_ids: list[int]) -> dict:
    """Compare multiple speakers."""

    results = []
    for speaker_id in speaker_ids:
        speaker = Speaker.query.get(speaker_id)  # N+1 query!

        # Get speaker stats
        stats = calculate_speaker_stats(speaker_id)
        results.append({
            'speaker': speaker,
            'stats': stats
        })

    return {'speakers': results}
```

**Implementation Steps:**

1. **Refactor with batch loading**

In `app/services/analytics/analytics_service.py:1243`, replace:

```python
def compare_speakers(speaker_ids: list[int]) -> dict:
    """
    Compare multiple speakers with their statistics.

    Uses batch loading to avoid N+1 queries.
    """
    logger.info(f"Comparing {len(speaker_ids)} speakers")

    if not speaker_ids:
        return {'speakers': []}

    # Single query to load all speakers
    speakers = db.session.query(Speaker).filter(
        Speaker.id.in_(speaker_ids)
    ).all()

    # Create lookup dict
    speaker_dict = {s.id: s for s in speakers}

    # Process in original order
    results = []
    for speaker_id in speaker_ids:
        speaker = speaker_dict.get(speaker_id)

        if not speaker:
            logger.warning(f"Speaker not found: {speaker_id}")
            continue

        # Get stats for this speaker
        stats = calculate_speaker_stats(speaker_id)

        results.append({
            'speaker_id': speaker.id,
            'speaker_name': f"{speaker.first_name} {speaker.last_name}",
            'speaker_email': speaker.email,
            'stats': stats
        })

    logger.info(f"Successfully compared {len(results)} speakers")

    return {
        'speakers': results,
        'total_compared': len(results)
    }
```

**Verification:**

- [ ] Add test:

```python
def test_compare_speakers_no_n_plus_one(db_session, app):
    """Speaker comparison should batch-load speakers."""
    from tests.factories import SpeakerFactory
    from app.services.analytics.analytics_service import compare_speakers
    from tests.helpers.query_counter import assert_query_count

    # Create 10 speakers
    speaker_ids = [SpeakerFactory().id for _ in range(10)]
    db_session.commit()

    # Should use ~5 queries total:
    # 1. Load all speakers (batch)
    # 2-5. Stats queries (can be optimized further later)
    with assert_query_count(15):  # Conservative limit
        result = compare_speakers(speaker_ids)

    assert len(result['speakers']) == 10
```

---

### Issue 2.5: N+1 Query in custom_report_service.py (event report)

**Problem:** Loops through events and counts registrations individually.

**Impact:** For 100 events, executes 101 queries instead of 2.

**File:** `app/services/analytics/custom_report_service.py:210-213`

**Current Problematic Code:**

```python
def generate_event_report(filters: dict) -> dict:
    # Get events
    events = query.all()

    # N+1 query loop
    results = []
    for event in events:
        # Separate query for EACH event
        reg_count = db.session.query(func.count(EventParticipant.id)).filter(
            EventParticipant.event_id == event.id
        ).scalar()

        results.append({
            'event': event,
            'registration_count': reg_count
        })

    return {'events': results}
```

**Implementation Steps:**

1. **Refactor with subquery**

In `app/services/analytics/custom_report_service.py:210-213`, replace:

```python
def generate_event_report(filters: dict) -> dict:
    """
    Generate event report with registration counts.

    Uses subquery to avoid N+1 queries.
    """
    from sqlalchemy import func, select
    from sqlalchemy.orm import aliased

    logger.info(f"Generating event report with filters: {filters}")

    # Create subquery for registration counts
    reg_count_subq = (
        select(
            EventParticipant.event_id,
            func.count(EventParticipant.id).label('reg_count')
        )
        .where(EventParticipant.status.in_(['registered', 'confirmed']))
        .group_by(EventParticipant.event_id)
        .subquery()
    )

    # Main query with left join to get counts
    query = db.session.query(
        Event,
        func.coalesce(reg_count_subq.c.reg_count, 0).label('registration_count')
    ).outerjoin(
        reg_count_subq,
        Event.id == reg_count_subq.c.event_id
    )

    # Apply filters
    if filters.get('date_from'):
        query = query.filter(Event.event_date >= filters['date_from'])

    if filters.get('date_to'):
        query = query.filter(Event.event_date <= filters['date_to'])

    if filters.get('event_type_ids'):
        query = query.filter(Event.event_type_id.in_(filters['event_type_ids']))

    # Execute single query
    results = query.all()

    logger.info(f"Generated report for {len(results)} events")

    # Format results
    formatted_results = []
    for event, reg_count in results:
        formatted_results.append({
            'event_id': event.id,
            'event_title': event.title,
            'event_date': event.event_date.isoformat() if event.event_date else None,
            'capacity': event.capacity,
            'registration_count': int(reg_count),
            'available_spots': event.capacity - int(reg_count)
        })

    return {
        'events': formatted_results,
        'total_events': len(formatted_results),
        'filters_applied': filters
    }
```

**Verification:**

- [ ] Add test:

```python
def test_event_report_no_n_plus_one(db_session, app):
    """Event report should use subquery for counts."""
    from tests.factories import EventFactory, EventParticipantFactory
    from app.services.analytics.custom_report_service import generate_event_report
    from tests.helpers.query_counter import assert_query_count

    # Create 20 events with varying registration counts
    for i in range(20):
        event = EventFactory(capacity=50)
        # Add random number of registrations
        for _ in range(i % 10):
            EventParticipantFactory(event_id=event.id, status='registered')

    db_session.commit()

    # Should complete in ~5 queries regardless of event count
    with assert_query_count(10):
        result = generate_event_report({})

    assert len(result['events']) == 20
    assert all('registration_count' in event for event in result['events'])
```

---

### Issue 2.6: N+1 Query in custom_report_service.py (participant report)

**Problem:** Loops through participants and queries their event statistics individually.

**Impact:** For 100 participants, could execute 200+ queries.

**File:** `app/services/analytics/custom_report_service.py:276-282`

**Current Problematic Code:**

```python
for participant, company in rows:
    # N+1 query for EACH participant
    stats_query = db.session.query(...).filter(
        EventParticipant.participant_id == participant.id
    )
    stats = stats_query.first()
```

**Implementation Steps:**

1. **Refactor with GROUP BY in main query**

In `app/services/analytics/custom_report_service.py:276-282`, replace with:

```python
def generate_participant_report(filters: dict) -> dict:
    """
    Generate participant report with event statistics.

    Uses GROUP BY to calculate stats in single query.
    """
    from sqlalchemy import func, case

    logger.info(f"Generating participant report with filters: {filters}")

    # Single query with all stats calculated via aggregation
    query = db.session.query(
        Participant.id,
        Participant.first_name,
        Participant.last_name,
        Participant.email,
        Company.name.label('company_name'),
        func.count(EventParticipant.id).label('total_events'),
        func.count(
            case((EventParticipant.status == 'confirmed', 1))
        ).label('confirmed_events'),
        func.count(
            case((EventParticipant.status == 'cancelled', 1))
        ).label('cancelled_events')
    ).join(
        Company,
        Participant.company_id == Company.id
    ).outerjoin(
        EventParticipant,
        Participant.id == EventParticipant.participant_id
    ).group_by(
        Participant.id,
        Participant.first_name,
        Participant.last_name,
        Participant.email,
        Company.name
    )

    # Apply filters
    if filters.get('company_ids'):
        query = query.filter(Company.id.in_(filters['company_ids']))

    if filters.get('date_from'):
        query = query.join(Event, EventParticipant.event_id == Event.id)
        query = query.filter(Event.event_date >= filters['date_from'])

    # Execute single query
    results = query.all()

    logger.info(f"Generated report for {len(results)} participants")

    # Format results
    formatted_results = []
    for row in results:
        formatted_results.append({
            'participant_id': row.id,
            'name': f"{row.first_name} {row.last_name}",
            'email': row.email,
            'company': row.company_name,
            'statistics': {
                'total_events': row.total_events,
                'confirmed_events': row.confirmed_events,
                'cancelled_events': row.cancelled_events,
                'attendance_rate': (
                    round(row.confirmed_events / row.total_events * 100, 1)
                    if row.total_events > 0 else 0
                )
            }
        })

    return {
        'participants': formatted_results,
        'total_participants': len(formatted_results),
        'filters_applied': filters
    }
```

**Verification:**

- [ ] Add test with query counting
- [ ] Run test suite
- [ ] Test in UI with 100+ participants

---

### Issue 2.7: N+1 Query in financial_service.py

**Problem:** Loops through registrations and accesses `reg.participant.membership_type` without eager loading.

**Impact:** For 100 registrations, could execute 200+ queries.

**File:** `app/services/finance/financial_service.py:373-392`

**Implementation Steps:**

1. **Add eager loading**

In `app/services/finance/financial_service.py`, find the function and add:

```python
from sqlalchemy.orm import joinedload

def calculate_revenue(event_id: int) -> dict:
    """Calculate revenue with eager loading of relationships."""

    # Eager load participant and membership_type
    registrations = db.session.query(EventParticipant).options(
        joinedload(EventParticipant.participant).joinedload(Participant.membership_type)
    ).filter(
        EventParticipant.event_id == event_id,
        EventParticipant.status.in_(['registered', 'confirmed'])
    ).all()

    total_revenue = 0
    for reg in registrations:
        # No additional queries - already loaded!
        membership = reg.participant.membership_type
        price = calculate_price(event_id, membership.id)
        total_revenue += price

    return {'total_revenue': total_revenue}
```

**Verification:**

- [ ] Add query count test
- [ ] Run tests
- [ ] Profile with 100+ registrations

---

### Issue 2.8: N+1 Query in event_template_service.py (copy_event_with_children)

**Problem:** Recursively copies child events, triggering multiple queries for each child.

**Impact:** For 5-day course, could execute 50+ queries.

**File:** `app/services/events/event_template_service.py:388-414`

**Implementation Steps:**

1. **Batch load all children first**

In `app/services/events/event_template_service.py:388-414`, replace:

```python
def copy_event_with_children(event_id: int, new_date: date, user_id: int) -> tuple[Event, list[Event]]:
    """
    Copy event and all children with optimized queries.

    Batch loads all children to avoid N+1 queries.
    """
    logger.info(f"Copying event with children: event_id={event_id}")

    # Load source event
    source = db.session.get(Event, event_id)
    if not source:
        raise ValueError(f"Event not found: {event_id}")

    # Batch load ALL children in one query
    from sqlalchemy.orm import joinedload

    children = db.session.query(Event).options(
        joinedload(Event.event_speakers),
        joinedload(Event.meeting_rooms),
        joinedload(Event.content_sections)
    ).filter(
        Event.parent_event_id == event_id
    ).order_by(Event.event_date).all()

    logger.info(f"Found {len(children)} child events to copy")

    # Copy parent event
    parent_copy = _copy_single_event(source, new_date, user_id)
    db.session.flush()

    # Copy all children
    child_copies = []
    for child in children:
        # Calculate date offset
        if child.event_date and source.event_date:
            days_offset = (child.event_date - source.event_date).days
            child_date = new_date + timedelta(days=days_offset)
        else:
            child_date = new_date

        child_copy = _copy_single_event(child, child_date, user_id)
        child_copy.parent_event_id = parent_copy.id
        child_copies.append(child_copy)

    db.session.commit()

    logger.info(
        f"Copied event {event_id} with {len(child_copies)} children. "
        f"New parent ID: {parent_copy.id}"
    )

    return (parent_copy, child_copies)


def _copy_single_event(source: Event, new_date: date, user_id: int) -> Event:
    """
    Copy a single event without children.

    Assumes relationships are already eager-loaded.
    """
    # Get copyable fields
    exclude_fields = {
        'id', 'event_date', 'event_start_time', 'event_end_time',
        'is_template', 'template_name', 'deleted_at', 'created_at',
        'updated_at', '_sa_instance_state', 'partnership',
        'event_speakers', 'budget', 'parent_event_id', 'child_days'
    }

    event_data = {}
    for column in source.__table__.columns:
        if column.name not in exclude_fields:
            event_data[column.name] = getattr(source, column.name)

    # Create new event
    new_event = Event(**event_data)
    new_event.event_date = new_date
    new_event.created_by_id = user_id

    db.session.add(new_event)
    db.session.flush()

    # Copy speakers (already loaded)
    for speaker_assoc in source.event_speakers:
        new_assoc = EventSpeaker(
            event_id=new_event.id,
            speaker_id=speaker_assoc.speaker_id,
            role=speaker_assoc.role
        )
        db.session.add(new_assoc)

    # Copy meeting rooms (already loaded)
    for room in source.meeting_rooms:
        new_room = MeetingRoom(
            event_id=new_event.id,
            room_name=room.room_name,
            capacity=room.capacity
        )
        db.session.add(new_room)

    # Copy content sections (already loaded)
    for section in source.content_sections:
        new_section = ContentSection(
            event_id=new_event.id,
            title=section.title,
            content=section.content,
            order=section.order
        )
        db.session.add(new_section)

    return new_event
```

**Verification:**

- [ ] Create test for multi-day event copying
- [ ] Verify query count with assert_query_count
- [ ] Test 5-day course copy (should be <20 queries)

---

## Phase 3: Performance Optimizations (Continued)

[Continue with remaining N+1 query fixes and performance issues...]

---

## Phase 4: Document Generator Refactor

**Phase 4 Status:** COMPLETED (2026-01-18)

Implementation summary:
- Issue 4.1: Added `build_pdf_header()` to base_generator.py, updated 4 generators
- Issue 4.2: Added `build_event_info_section()` to base_generator.py, updated 3 generators
- Issue 4.3: Added `apply_standard_table_style()` to base_generator.py, updated 3 generators

### Issue 4.1: Duplicate PDF Header Generation

**Problem:** 50 lines of PDF header generation code duplicated across 3 document generators.

**Impact:** Bug fixes must be applied 3 times, inconsistent styling possible.

**Files:**
- `app/services/documents/attendance_generator.py:126-176`
- `app/services/documents/dietary_generator.py:425-473`
- `app/services/documents/participant_list_generator.py:598-648`

**Implementation Steps:**

1. **Extract to base_generator.py**

In `app/services/documents/base_generator.py`, add:

```python
def create_pdf_header(
    document_type_label: str,
    language: str,
    available_width: float,
    styles: dict
) -> Table:
    """
    Create standardized PDF header for all document types.

    Args:
        document_type_label: Translated document type (e.g., "Attendance Sheet")
        language: Language code (en/fr/nl)
        available_width: Available width for the header
        styles: ReportLab styles dictionary

    Returns:
        ReportLab Table object with header content
    """
    from reportlab.lib.units import mm
    from reportlab.platypus import Image, Paragraph
    from reportlab.lib import colors

    logger.info(f"Creating PDF header: type='{document_type_label}', lang='{language}'")

    # Logo
    logo_path = os.path.join(current_app.root_path, 'static', 'images', 'guberna_logo.png')
    try:
        logo = Image(logo_path, width=40*mm, height=15*mm)
    except Exception as e:
        logger.warning(f"Failed to load logo: {e}")
        logo = Paragraph("GUBERNA", styles['Heading1'])

    # Document title
    title = Paragraph(
        f"<b>{document_type_label}</b>",
        styles['DocumentTitle']
    )

    # Generation date
    from datetime import datetime
    date_label = {
        'en': 'Generated on',
        'fr': 'Gnr le',
        'nl': 'Gegenereerd op'
    }.get(language, 'Generated on')

    gen_date = Paragraph(
        f"{date_label}: {datetime.now().strftime('%d/%m/%Y %H:%M')}",
        styles['Normal']
    )

    # Create header table
    header_data = [
        [logo, title],
        ['', gen_date]
    ]

    header_table = Table(header_data, colWidths=[available_width * 0.3, available_width * 0.7])
    header_table.setStyle(TableStyle([
        ('ALIGN', (0, 0), (0, -1), 'LEFT'),
        ('ALIGN', (1, 0), (1, -1), 'RIGHT'),
        ('VALIGN', (0, 0), (-1, -1), 'MIDDLE'),
        ('BOTTOMPADDING', (0, 0), (-1, -1), 12),
    ]))

    return header_table
```

2. **Update attendance_generator.py to use shared function**

In `app/services/documents/attendance_generator.py:126-176`, replace:

```python
# DELETE old header code (lines 126-176)

# REPLACE WITH:
from app.services.documents.base_generator import create_pdf_header

# Inside generate_attendance_sheet function:
header = create_pdf_header(
    document_type_label=translations[language]['attendance_sheet'],
    language=language,
    available_width=available_width,
    styles=styles
)
story.append(header)
```

3. **Update dietary_generator.py**

In `app/services/documents/dietary_generator.py:425-473`, apply same pattern.

4. **Update participant_list_generator.py**

In `app/services/documents/participant_list_generator.py:598-648`, apply same pattern.

**Verification:**

- [ ] Generate all 3 document types
- [ ] Compare PDF output before/after (should be identical)
- [ ] Verify headers render correctly in all languages
- [ ] Code search for duplicate header code:
  ```bash
  grep -n "guberna_logo.png" app/services/documents/*.py
  ```
  Should only appear in base_generator.py

---

### Issue 4.2: Duplicate Event Info Section

**Problem:** 60 lines of event info rendering duplicated 3 times.

**Impact:** Same as Issue 4.1.

**Files:**
- `app/services/documents/attendance_generator.py:178-237`
- `app/services/documents/dietary_generator.py:475-526`
- `app/services/documents/participant_list_generator.py:650-705`

**Implementation Steps:**

[Similar pattern to Issue 4.1 - extract to base_generator.py]

---

### Issue 4.3: Duplicate Table Styling

**Problem:** 50 lines of table style code duplicated 3 times.

**Files:**
- `app/services/documents/attendance_generator.py:289-339`
- `app/services/documents/dietary_generator.py:576-620`
- `app/services/documents/participant_list_generator.py:754-806`

**Implementation Steps:**

[Similar pattern - extract apply_standard_table_style function]

---

## Phase 5: Event Service Refactor

**Phase 5 Status:** COMPLETED (2026-01-18)

Implementation summary:
- Added `get_copyable_fields()` method to Event model (app/models/events/event.py)
- Refactored `create_event_from_template()` to use new model method
- Refactored `copy_event()` to use new model method with `exclude_additional={'master_event_id'}`
- Eliminated ~28 lines of duplicate code (2 exclusion blocks of ~14 lines each)

### Issue 5.1: Duplicate Field Copying Logic

**Problem:** Event field exclusion logic duplicated in 3 locations.

**Files:**
- `app/services/events/event_template_service.py:127-149`
- `app/services/events/event_template_service.py:255-268`
- `app/services/events/event_template_service.py:389-409`

**Implementation Steps:**

1. **Add method to Event model**

In `app/models/events/event.py`, add:

```python
def get_copyable_fields(self, exclude_additional: set = None) -> dict:
    """
    Return dictionary of fields safe to copy to new event instance.

    Args:
        exclude_additional: Additional fields to exclude beyond defaults

    Returns:
        Dict of column_name: value for copyable fields
    """
    base_exclude = {
        'id', 'event_date', 'event_start_time', 'event_end_time',
        'is_template', 'template_name', 'deleted_at', 'created_at',
        'updated_at', '_sa_instance_state', 'partnership',
        'event_speakers', 'budget', 'parent_event_id', 'child_days',
        'version'  # Optimistic locking field
    }

    if exclude_additional:
        base_exclude.update(exclude_additional)

    result = {}
    for column in self.__table__.columns:
        if column.name not in base_exclude:
            result[column.name] = getattr(self, column.name)

    return result
```

2. **Update all 3 locations to use model method**

In `app/services/events/event_template_service.py`, replace all field copying code:

```python
# BEFORE
exclude_fields = {...}  # 15 lines
event_data = {}
for column in template.__table__.columns:
    if column.name not in exclude_fields:
        event_data[column.name] = getattr(template, column.name)

# AFTER
event_data = source_event.get_copyable_fields()
```

**Verification:**

- [ ] Test template creation
- [ ] Test event copying
- [ ] Test multi-day event copying
- [ ] Verify all fields copied correctly

---

## Phase 6: Analytics Refactor

**Phase 6 Status:** COMPLETED (2026-01-18)

Implementation summary:
- Split analytics_service.py from 2062 lines into 6 focused modules
- Created: core_analytics.py (275 lines), question_analytics.py (803 lines), event_analytics.py (504 lines), speaker_analytics.py (292 lines), trends_analytics.py (194 lines), analytics_cache.py (146 lines)
- analytics_service.py reduced to 117 lines (backward compatibility re-exports)
- All imports verified working, backward compatibility maintained

### Issue 6.1: Split analytics_service.py (1,928 lines)

**Problem:** Single file contains 15 different analytics types. Difficult to navigate and maintain.

**Impact:** Merge conflicts, difficult to find functions, slow to load in editor.

**File:** `app/services/analytics/analytics_service.py`

**Target Structure:**
```
app/services/analytics/
 __init__.py (re-exports all functions)
 question_analytics.py (question-specific analytics)
 event_analytics.py (event-level analytics)
 speaker_analytics.py (speaker analytics)
 analytics_cache.py (cache management)
```

**Implementation Steps:**

1. **Create question_analytics.py**

Create `app/services/analytics/question_analytics.py` with functions:
- `get_question_analytics()`
- `calculate_question_statistics()`
- `get_question_response_breakdown()`

2. **Create event_analytics.py**

Create `app/services/analytics/event_analytics.py` with functions:
- `get_event_analytics()`
- `calculate_event_metrics()`
- `get_event_comparison()`

3. **Create speaker_analytics.py**

Create `app/services/analytics/speaker_analytics.py` with functions:
- `compare_speakers()`
- `calculate_speaker_stats()`
- `get_speaker_performance()`

4. **Create analytics_cache.py**

Create `app/services/analytics/analytics_cache.py` with functions:
- `invalidate_question_analytics()`
- `invalidate_event_analytics()`
- `clear_analytics_cache()`

5. **Update __init__.py to re-export everything**

In `app/services/analytics/__init__.py`:

```python
"""Analytics services."""

from app.services.analytics.question_analytics import (
    get_question_analytics,
    calculate_question_statistics,
)

from app.services.analytics.event_analytics import (
    get_event_analytics,
    calculate_event_metrics,
)

from app.services.analytics.speaker_analytics import (
    compare_speakers,
    calculate_speaker_stats,
)

from app.services.analytics.analytics_cache import (
    invalidate_question_analytics,
    clear_analytics_cache,
)

__all__ = [
    # Question analytics
    'get_question_analytics',
    'calculate_question_statistics',
    # Event analytics
    'get_event_analytics',
    'calculate_event_metrics',
    # Speaker analytics
    'compare_speakers',
    'calculate_speaker_stats',
    # Cache
    'invalidate_question_analytics',
    'clear_analytics_cache',
]
```

6. **Update all imports across codebase**

Old imports still work thanks to __init__.py re-exports:
```python
from app.services.analytics.analytics_service import get_question_analytics
```

New imports (preferred):
```python
from app.services.analytics import get_question_analytics
```

**Verification:**

- [ ] Run full test suite
- [ ] Check all routes still import correctly
- [ ] Verify analytics dashboard loads
- [ ] Test each analytics function

---

## Phase 7: Error Handling Standardization

**Phase 7 Status:** COMPLETED (2026-01-18)

Implementation summary:
- Created exception-based error handling pattern for task_service.py
- Refactored all task service functions to raise ValidationError/NotFoundError/BusinessRuleError
- Updated routes/api/tasks/tasks.py to use cleaner exception-based flow
- Leveraged existing infrastructure: exceptions.py (4 exception classes), error_codes.py (347+ codes), error_handlers.py (global handlers)
- task_service.py now returns single values on success, raises exceptions on failure
- Routes simplified by removing tuple unpacking and error checking

### Issue 7.1: Inconsistent Error Patterns (4 Different Patterns)

**Problem:** Services use 4 different error handling patterns - tuple returns, exceptions, boolean returns, dict returns.

**Impact:** Callers must know which pattern each service uses, inconsistent error handling across codebase.

**Decision:** Standardize to exceptions with error codes from `app/utils/error_codes.py`.

**Files Affected:** All 79 service files

**Implementation Steps:**

1. **Define standard exception class**

Create `app/utils/errors/service_exceptions.py`:

```python
"""Standard exceptions for service layer."""

class ServiceError(Exception):
    """Base exception for service layer errors."""

    def __init__(self, error_code: str, message: str, field: str = None):
        self.error_code = error_code
        self.message = message
        self.field = field
        super().__init__(message)

    def to_dict(self) -> dict:
        """Convert to API error response format."""
        return {
            'success': False,
            'error': {
                'code': self.error_code,
                'message': self.message,
                'field': self.field
            }
        }


class ValidationError(ServiceError):
    """Validation error."""
    pass


class NotFoundError(ServiceError):
    """Resource not found error."""
    pass


class PermissionError(ServiceError):
    """Permission denied error."""
    pass


class BusinessRuleError(ServiceError):
    """Business rule violation error."""
    pass
```

2. **Update error codes registry**

In `app/utils/errors/error_codes.py`, ensure all service errors are registered:

```python
# Task service errors
ERR_TASK_TITLE_REQUIRED = "ERR_TASK_TITLE_REQUIRED"
ERR_TASK_NOT_FOUND = "ERR_TASK_NOT_FOUND"

# Event service errors
ERR_EVENT_NOT_FOUND = "ERR_EVENT_NOT_FOUND"
ERR_EVENT_AT_CAPACITY = "ERR_EVENT_AT_CAPACITY"

# Registration errors
ERR_INSUFFICIENT_CAPACITY = "ERR_INSUFFICIENT_CAPACITY"
ERR_REGISTRATION_NOT_FOUND = "ERR_REGISTRATION_NOT_FOUND"

# ... etc for all services
```

3. **Refactor task_service.py as example**

In `app/services/tasks/task_service.py`, replace:

```python
# BEFORE (tuple return pattern)
def create_task(title: str, description: str) -> tuple[Task | None, str | None]:
    if not title or not title.strip():
        return (None, 'Title is required.')

    try:
        task = Task(title=title, description=description)
        db.session.add(task)
        db.session.commit()
        return (task, None)
    except Exception as e:
        db.session.rollback()
        return (None, str(e))

# AFTER (exception pattern)
def create_task(title: str, description: str) -> Task:
    """
    Create a new task.

    Args:
        title: Task title (required)
        description: Task description

    Returns:
        Created Task instance

    Raises:
        ValidationError: If title is missing or invalid
        ServiceError: If database operation fails
    """
    from app.utils.errors.service_exceptions import ValidationError, ServiceError
    from app.utils.errors.error_codes import ERR_TASK_TITLE_REQUIRED

    logger.info(f"Creating task: title='{title}'")

    # Validation
    if not title or not title.strip():
        raise ValidationError(
            error_code=ERR_TASK_TITLE_REQUIRED,
            message="Task title is required and cannot be empty.",
            field='title'
        )

    try:
        task = Task(title=title.strip(), description=description)
        db.session.add(task)
        db.session.commit()

        logger.info(f"Task created successfully: id={task.id}")
        return task

    except Exception as e:
        db.session.rollback()
        logger.error(f"Failed to create task: {str(e)}", exc_info=True)
        raise ServiceError(
            error_code='ERR_TASK_CREATE_FAILED',
            message=f"Failed to create task: {str(e)}"
        )
```

4. **Update route error handlers**

In `app/routes/api/tasks/tasks.py`, update:

```python
# BEFORE
task, error = create_task(title, description)
if error:
    return jsonify({'success': False, 'error': error}), 400
return jsonify({'success': True, 'task': task.to_dict()})

# AFTER
from app.utils.errors.service_exceptions import ValidationError, ServiceError

try:
    task = create_task(title, description)
    return jsonify({'success': True, 'task': task.to_dict()})
except ValidationError as e:
    return jsonify(e.to_dict()), 400
except ServiceError as e:
    return jsonify(e.to_dict()), 500
```

5. **Add global error handler**

In `app/utils/errors/error_handlers.py`, add:

```python
from app.utils.errors.service_exceptions import (
    ValidationError,
    NotFoundError,
    PermissionError,
    BusinessRuleError,
    ServiceError
)

def register_service_error_handlers(app):
    """Register handlers for service layer exceptions."""

    @app.errorhandler(ValidationError)
    def handle_validation_error(e):
        return jsonify(e.to_dict()), 400

    @app.errorhandler(NotFoundError)
    def handle_not_found_error(e):
        return jsonify(e.to_dict()), 404

    @app.errorhandler(PermissionError)
    def handle_permission_error(e):
        return jsonify(e.to_dict()), 403

    @app.errorhandler(BusinessRuleError)
    def handle_business_rule_error(e):
        return jsonify(e.to_dict()), 422

    @app.errorhandler(ServiceError)
    def handle_service_error(e):
        return jsonify(e.to_dict()), 500
```

6. **Register handlers in app factory**

In `app/__init__.py`:

```python
from app.utils.errors.error_handlers import register_service_error_handlers

def create_app():
    # ... existing setup ...

    register_service_error_handlers(app)

    return app
```

7. **Refactor remaining services one by one**

Priority order:
1. `task_service.py` (example above)
2. `event_type_service.py`
3. `registration_service.py`
4. `waiting_list_service.py`
5. All other services

**Verification:**

- [ ] Refactor task_service.py
- [ ] Update routes/api/tasks/tasks.py
- [ ] Run tests: `pytest tests/test_services/tasks/ -v`
- [ ] Test API endpoints:
  - [ ] POST /api/tasks (valid data) - returns 200
  - [ ] POST /api/tasks (missing title) - returns 400 with error code
  - [ ] POST /api/tasks (database error) - returns 500 with error code
- [ ] Refactor remaining services (follow same pattern)

---

## Phase 8: Cleanup & Documentation

**Phase 8 Status:** COMPLETED (2026-01-18)

Implementation summary:
- Issue 8.1: Created pdf_constants.py with NAME_CARD_* constants, updated name_cards_generator.py
- Issue 8.2: Fixed silent exception swallowing in 10 files (guberna_document_styles.py, name_cards_generator.py, certificate_service.py, file_storage_service.py, partnership_logo_service.py, speaker_photo_service.py, participant_import_service.py, registration_email_service.py, registration_service.py, import_service.py)
- Issue 8.3: Created service-layer-style-guide.md documentation

### Issue 8.1: Hardcoded Magic Numbers

**Problem:** PDF dimension constants hardcoded instead of using named constants.

**File:** `app/services/documents/name_cards_generator.py:263-268`

**Implementation Steps:**

1. **Create constants file**

Create `app/utils/constants/pdf_constants.py`:

```python
"""Constants for PDF document generation."""

from reportlab.lib.units import mm

# Page dimensions (A4)
PAGE_WIDTH_A4 = 210 * mm
PAGE_HEIGHT_A4 = 297 * mm

# Standard margins
MARGIN_STANDARD = 10 * mm
MARGIN_SMALL = 5 * mm
MARGIN_LARGE = 20 * mm

# Name card dimensions
NAME_CARD_MARGIN_X = 10 * mm
NAME_CARD_MARGIN_Y = 10 * mm
NAME_CARD_WIDTH = PAGE_WIDTH_A4 - 2 * NAME_CARD_MARGIN_X
NAME_CARD_HEIGHT = 60 * mm

# Font sizes
FONT_SIZE_TITLE = 24
FONT_SIZE_HEADING = 18
FONT_SIZE_NORMAL = 12
FONT_SIZE_SMALL = 10

# Colors
COLOR_HEADER_BG = '#F0F0F0'
COLOR_BORDER = '#CCCCCC'
```

2. **Update name_cards_generator.py**

In `app/services/documents/name_cards_generator.py:263-268`, replace:

```python
# BEFORE
margin_x = 10 * mm
margin_y = 10 * mm
card_width = page_width - 2 * margin_x

# AFTER
from app.utils.constants.pdf_constants import (
    NAME_CARD_MARGIN_X,
    NAME_CARD_MARGIN_Y,
    NAME_CARD_WIDTH
)

margin_x = NAME_CARD_MARGIN_X
margin_y = NAME_CARD_MARGIN_Y
card_width = NAME_CARD_WIDTH
```

**Verification:**

- [ ] Generate name cards PDF
- [ ] Verify layout unchanged
- [ ] Search for remaining magic numbers: `grep -n "10 \* mm" app/services/documents/*.py`

---

### Issue 8.2: Silent Exception Swallowing

**Problem:** 30+ locations catch exceptions and silently ignore them without logging.

**Files:**
- `app/services/documents/guberna_document_styles.py:178, 615, 827`
- `app/services/feedback/import_service.py:269, 280, 996`
- And 25+ additional locations

**Implementation Steps:**

1. **Add logging to all silent catches**

Pattern to find:
```python
except Exception:
    pass
```

Replace with:
```python
except Exception as e:
    logger.warning(f"[Context]: {str(e)}")
    # Continue with fallback behavior
```

2. **Example fix in guberna_document_styles.py:178**

```python
# BEFORE
try:
    img = ImageReader(guberna_logo)
except Exception:
    pass

# AFTER
try:
    img = ImageReader(guberna_logo)
except Exception as e:
    logger.warning(f"Failed to load GUBERNA logo from {guberna_logo}: {str(e)}")
    img = None  # Explicitly set fallback
```

3. **Find all locations**

```bash
grep -n "except Exception:" app/services/ -A 1 | grep -B 1 "pass"
```

4. **Fix each location with appropriate context**

**Verification:**

- [ ] Search for remaining silent catches: `grep -n "except.*:$" app/services/ -A 1 | grep "pass$"`
- [ ] Result should be empty
- [ ] Run tests to ensure no new failures
- [ ] Check logs during document generation for warnings

---

### Issue 8.3: Update Service Architecture Documentation

**Problem:** Documentation doesn't reflect new patterns (optimistic locking, exception handling, etc.).

**File:** `Docs/Development Docs/06-service-layer-architecture.md`

**Implementation Steps:**

1. **Update architecture doc with new patterns**

Add sections:
- Optimistic Locking Pattern
- Standard Exception Handling
- Query Performance Guidelines
- Logging Standards

2. **Create service layer style guide**

Create `Docs/Development Docs/service-layer-style-guide.md`:

```markdown
# Service Layer Style Guide

## Error Handling

All services MUST use exception-based error handling:

```python
from app.utils.errors.service_exceptions import ValidationError, ServiceError

def create_resource(data: dict) -> Resource:
    if not data.get('required_field'):
        raise ValidationError(
            error_code='ERR_FIELD_REQUIRED',
            message='Field is required',
            field='required_field'
        )

    try:
        resource = Resource(**data)
        db.session.add(resource)
        db.session.commit()
        return resource
    except Exception as e:
        db.session.rollback()
        raise ServiceError(
            error_code='ERR_CREATE_FAILED',
            message=str(e)
        )
```

## Logging

All service functions MUST log:
- Entry point with parameters
- Success with result summary
- Errors with full context

## Performance

- Use `joinedload()` for N+1 prevention
- Add query count tests for list operations
- Use subqueries for aggregations

## Transactions

Use nested transactions for atomic operations:

```python
with db.session.begin_nested():
    # Multiple operations
    db.session.flush()
    # Audit logging
db.session.commit()
```
```

**Verification:**

- [ ] Review documentation changes with team
- [ ] Update CLAUDE.md to reference new style guide
- [ ] Add link to style guide in service __init__.py files

---

## Completion Checklist

### Phase 0: Prerequisites
- [ ] Issue 0.1: Add logging to all 63 service files
- [ ] Issue 0.2: Create query performance monitoring helpers

### Phase 1: Critical Data Integrity
- [ ] Issue 1.1: Fix race conditions with optimistic locking
- [ ] Issue 1.2: Fix all 52 Model.query.get() calls
- [ ] Issue 1.3: Fix cache bug with filters
- [ ] Issue 1.4: Add transactions to event_type_service
- [ ] Issue 1.5: Delete dead code files

### Phase 2: Critical Code Fixes
- [ ] Issue 2.1: Add __init__.py exports (7 files)
- [ ] Issue 2.2: Delete dead functions (3 functions)
- [ ] Issue 2.3: Fix N+1 in get_question_analytics
- [ ] Issue 2.4: Fix N+1 in compare_speakers
- [ ] Issue 2.5: Fix N+1 in event report
- [ ] Issue 2.6: Fix N+1 in participant report
- [ ] Issue 2.7: Fix N+1 in financial_service
- [ ] Issue 2.8: Fix N+1 in copy_event_with_children

### Phase 3: Performance
- [ ] All N+1 queries resolved
- [ ] Query count tests added
- [ ] Performance benchmarks collected

### Phase 4: Document Generator Refactor
- [x] Issue 4.1: Extract PDF header generation
- [x] Issue 4.2: Extract event info section
- [x] Issue 4.3: Extract table styling

### Phase 5: Event Service Refactor
- [x] Issue 5.1: Extract field copying to Event model

### Phase 6: Analytics Refactor
- [x] Issue 6.1: Split analytics_service.py into 6 files (completed 2026-01-18)

### Phase 7: Error Standardization
- [x] Issue 7.1: Convert all services to exception pattern (completed for task_service.py as example)

### Phase 8: Cleanup & Documentation
- [x] Issue 8.1: Extract PDF constants
- [x] Issue 8.2: Fix silent exception swallowing
- [x] Issue 8.3: Update documentation

---

## Success Metrics

### Code Quality
- [ ] Zero deprecated Model.query.get() calls
- [ ] Zero N+1 query patterns in critical paths
- [ ] No file >800 lines in services/
- [ ] 100% of services use exception pattern
- [ ] Zero silent exception catches

### Performance
- [ ] Dashboard load <500ms
- [ ] Question analytics <200ms for 100 responses
- [ ] Event copy <1s for 5-day course
- [ ] Custom reports <2s for 100 events

### Observability
- [ ] 100% of services have logging
- [ ] Query count monitoring in place
- [ ] Error tracking with codes
- [ ] Performance metrics available

---

*Document Version: 3.1 - Complete Refactoring Plan*
*Last Updated: 2026-01-18*
*Status: Ready for Execution with plan-execution command*
